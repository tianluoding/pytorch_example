{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import Embedding\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2])\n",
    "          for i in range(len(test_sentence)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个单词编码\n",
    "vocb = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define N Gram\n",
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        self.embedding = Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size*n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1)\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_prob = F.log_softmax(out, dim=1)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NgramModel(len(vocb), CONTEXT_SIZE, EMBEDDING_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ngram.parameters(), lr=1e-2, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.987432937706466\n",
      "3.3540546731875005\n",
      "1.0814985939885173\n",
      "0.3864251757067341\n",
      "0.09810662657415313\n",
      "0.0555082476625167\n",
      "0.049916519789795666\n",
      "0.0467532854375277\n",
      "0.04533410951977863\n",
      "0.05223839276770812\n",
      "0.058881534539784855\n",
      "0.04718340065334967\n",
      "0.04233097136902146\n",
      "0.040502370365134766\n",
      "0.04267641846746679\n",
      "0.040838821730812117\n",
      "0.041138941299137935\n",
      "0.050163046100576855\n",
      "0.07570814660544156\n",
      "0.04283310641323173\n",
      "0.04976670081089498\n",
      "0.04487244128411846\n",
      "0.03754658735594434\n",
      "0.04062092941062756\n",
      "0.03921988525580351\n",
      "0.03743961033924118\n",
      "0.03799236347659056\n",
      "0.038288960620952536\n",
      "0.0363403348315536\n",
      "0.03964861650514392\n",
      "0.03705331598461577\n",
      "0.0376815433317509\n",
      "0.03891293756639074\n",
      "0.042401417272898936\n",
      "0.05571716320869643\n",
      "0.13054617703042357\n",
      "0.401549558211579\n",
      "3.206679124232561\n",
      "1.7059362919301866\n",
      "0.0654851120593001\n",
      "0.040582053151880536\n",
      "0.03798523969704323\n",
      "0.036732343356939645\n",
      "0.03593573503259997\n",
      "0.03540064340770883\n",
      "0.03502691860275169\n",
      "0.0347672845844368\n",
      "0.03459669668803035\n",
      "0.03600070457723593\n",
      "0.03453681051609876\n",
      "0.03432453880673887\n",
      "0.034239422373522684\n",
      "0.03419636839680814\n",
      "0.03417269270468002\n",
      "0.03415976845087516\n",
      "0.034152936449174644\n",
      "0.03416224551064286\n",
      "0.034221143488915015\n",
      "0.03415328218865813\n",
      "0.03416116692345459\n",
      "0.03414820047936165\n",
      "0.034143722455230216\n",
      "0.034133874947504375\n",
      "0.03412462455192923\n",
      "0.034114741618536004\n",
      "0.03414356401317348\n",
      "0.03407864504937443\n",
      "0.0340779693346475\n",
      "0.03416470616924272\n",
      "0.03399630319686049\n",
      "0.0340511088756935\n",
      "0.034000309023488655\n",
      "0.033962400200977766\n",
      "0.033942866060362065\n",
      "0.03390691062895033\n",
      "0.033881875091229835\n",
      "0.033854883222312185\n",
      "0.0338112887897647\n",
      "0.03385500212426993\n",
      "0.033779966757762236\n",
      "0.03363977330342843\n",
      "0.03393063634141977\n",
      "0.03711775178567266\n",
      "0.03674504655375293\n",
      "0.05405405385959433\n",
      "0.0414039094750681\n",
      "0.0879772103810852\n",
      "0.09759009910553496\n",
      "0.03662573355991922\n",
      "0.033121187703043245\n",
      "0.03221086361492645\n",
      "0.0321970862189774\n",
      "0.03222499851642316\n",
      "0.03221536585849918\n",
      "0.0322383227775814\n",
      "0.0322814494234759\n",
      "0.03233091272242671\n",
      "0.03236393590291888\n",
      "0.032418029162562936\n",
      "0.03243360280894657\n",
      "0.03254152754779164\n",
      "0.032501144900072734\n",
      "0.032694779485022064\n",
      "0.03246261672793923\n",
      "0.03249613641018975\n",
      "0.03710118039307659\n",
      "0.03382364841026118\n",
      "0.03546492948032765\n",
      "0.07072907956076574\n",
      "0.112462426032896\n",
      "2.671169262684213\n",
      "2.07754768443849\n",
      "0.7682061234895663\n",
      "0.060798316948738516\n",
      "0.03391864162133638\n",
      "0.032409403108165515\n",
      "0.031881403826884605\n",
      "0.031608482388486334\n",
      "0.031476651387038715\n",
      "0.03143250035765868\n",
      "0.03140810724217128\n",
      "0.03143670290360886\n",
      "0.03149926460515067\n",
      "0.03152543242510801\n",
      "0.03155801916354176\n",
      "0.03160630486243287\n",
      "0.031634058542470575\n",
      "0.03168159838446514\n",
      "0.03170740749841175\n",
      "0.03175387689434437\n",
      "0.031767504969826166\n",
      "0.03183815602542818\n",
      "0.03179352197827294\n",
      "0.032032056178017854\n",
      "0.03286564942918441\n",
      "0.03220900502563998\n",
      "0.03180192047976819\n",
      "0.03223778411778325\n",
      "0.03210406773032744\n",
      "0.03168744976610937\n",
      "0.03521102617270935\n",
      "0.032317727130055346\n",
      "0.03454580252273065\n",
      "0.031539598995358756\n",
      "0.032618132835402866\n",
      "0.03179850878819374\n",
      "0.0318375858443673\n",
      "0.03565472467049108\n",
      "0.04592192004338842\n",
      "0.031623300718874384\n",
      "0.06882680826494203\n",
      "0.03731007930643551\n",
      "0.03835659946101466\n",
      "0.0639783321406356\n",
      "0.03617632970491048\n",
      "0.040159545996371185\n",
      "0.030666254234102644\n",
      "0.03068875317963915\n",
      "0.03069871083724088\n",
      "0.030763263941874047\n",
      "0.03085890349882416\n",
      "0.03092467983915264\n",
      "0.0310195585428963\n",
      "0.03107106857756018\n",
      "0.03112914261347142\n",
      "0.031179057937624606\n",
      "0.03123316851222728\n",
      "0.031271067292241286\n",
      "0.031313012297773384\n",
      "0.031338696941671185\n",
      "0.03136994342378726\n",
      "0.03138545212034583\n",
      "0.031410303761828066\n",
      "0.03140987008942121\n",
      "0.03144030249452841\n",
      "0.03141805573679813\n",
      "0.03145211142278455\n",
      "0.03146482213335215\n",
      "0.03156628766346048\n",
      "0.03174914937789358\n",
      "0.03149448465132508\n",
      "0.058912015341681445\n",
      "0.0700473864583614\n",
      "0.03216637782627828\n",
      "0.03183712126501914\n",
      "0.03144107263064967\n",
      "0.04578663645458867\n",
      "0.03342625054858949\n",
      "0.040498717921818195\n",
      "0.030034436528468777\n",
      "0.10123004905306192\n",
      "1.3080141108763388\n",
      "2.3918174003508286\n",
      "0.43716602511046165\n",
      "0.1588660879453905\n",
      "0.034442536399503655\n",
      "0.03108711986176955\n",
      "0.030107112593096445\n",
      "0.030028208144655465\n",
      "0.030029808977161565\n"
     ]
    }
   ],
   "source": [
    "epoch = 200\n",
    "for e in range(epoch):\n",
    "    train_loss = 0\n",
    "    for word, label in trigram:\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        output = ngram(word)\n",
    "        loss = criterion(output, label)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(train_loss/len(trigram))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real word is beauty's, predict word is beauty's\n"
     ]
    }
   ],
   "source": [
    "word, label = trigram[11]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngram(word)\n",
    "_, predict = torch.max(out, 1)\n",
    "predict_word = idx_to_word[predict.item()]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
