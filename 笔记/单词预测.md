# 基于N Gram的单词预测

* 基于N Gram语言模型的单词预测
* pytorch 1.9

## Word Embedding

* 词向量（Word embedding）：使用一个高维向量去表示一个单词，每一位都是实数，这些实数隐含着单词的某种属性。

* 特点：不同词向量之间的夹角，表示不同词之间的相似程度。

  ### pytorch实现词向量

  * nn.Embedding(m, n) 其中m表示单词数目，n表示词向量维度

  * 基本步骤：1.对单词编码，即用数字表示；2.将tensor转换成Variable，因为词向量也是网络更新的参数，在计算图中。

  * ```python
    word_to_idx = {'hello': 0, 'world': 1}
    hello_idx = torch.LongTensor([word_to_idx['hello']])
    hello_idx = Variable(hello_idx)
    ```

## N Gram语言模型

* 引入了马尔科夫假设，假设这个单词只与前面的几个单词有关系

  ### 定义 N Gram

  ```python
  class NgramModel(nn.Module):
      def __init__(self, vocb_size, context_size, n_dim): #单词数，依赖单词数，词向量维度
  		super(NgramModel, self).__init__()
          self.n_word = vocb_size
          self.embedding = nn.Embedding(self.n_word, n_dim)
          self.linear1 = nn.Linear(context_size*n_dim, 128)
          self.linear2 = nn.Linear(128, n_word)
          
      def forward(x):
          emb = self.embedding(x)
          emb = emb.view(1, -1)
          out = self.linear1(emb)
          out = F.relu(out)
          out = self.linear2(out)
          log_prob = F.log_softmax(out, dim=1)
          return log_prob
  ```

* **softmax和log_softmax原理**

  softmax，指数标准化函数，公式：第i个z求指数之后，除以所以z的求指数之和。在概率论中，softmax代表类别分布，也就是不同可能结果的概率分布。
  $$
  \sigma(z) = e^{z_i}/\sum^K_{j=1}e^{z_j}\ for\  i=1,...,K\ and\ z=(z_1,...,z_K)
  $$
  log_softmax就是对softmax函数取对数

  关于softmax的dim参数，dim=0列和为1，dim=1行和为1



## 训练集

```python
# train dataset
CONTEXT_SIZE = 2
EMBEDDING_DIM = 10

test_sentence = """When forty winters shall besiege thy brow,
And dig deep trenches in thy beauty's field,
Thy youth's proud livery so gazed on now,
Will be a totter'd weed of small worth held:
Then being asked, where all thy beauty lies,
Where all the treasure of thy lusty days;
To say, within thine own deep sunken eyes,
Were an all-eating shame, and thriftless praise.
How much more praise deserv'd thy beauty's use,
If thou couldst answer 'This fair child of mine
Shall sum my count, and make my old excuse,'
Proving his beauty by succession thine!
This were to be new made when thou art old,
And see thy blood warm when thou feel'st it cold.""".split()
```



## 单词编码

* 将训练集分为三部分

  ```python
  trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2]) for i in range(len(test_sentence)-2)]
  ```

* 将单词编码

  ```python
  vocb = set(test_sentence)
  word_to_idx = {word: i for i, word in enumerate(vocb)}
  idx_to_word = {word_to_idx[word]: word for word in word_to_idx}
  ```

  

## 训练过程

```python
ngram = NgramModel(len(vocb), CONTEXT_SIZE, EMBEDDING_DIM)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(ngram.parameters(), lr=1e-2, weight_decay=1e-5)

epoch = 200
for e in range(epoch):
    train_loss = 0
    for word, label in trigram:
        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))
        label = Variable(torch.LongTensor([word_to_idx[label]]))
        output = ngram(word)
        loss = criterion(output, label)
        train_loss += loss.item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    print(train_loss/len(trigram))       
```

